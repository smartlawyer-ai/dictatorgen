import json
from typing import Any, AsyncGenerator, Dict, List
import logging
from dictatorgenai.agents.general import General
from dictatorgenai.models.base_model import BaseModel, Message
from dictatorgenai.steps.action_steps import PlanningStep
from dictatorgenai.utils.task import Task


class ColonelFragmenter(General):
    def __init__(
        self,
        my_name_is: str,
        iam: str,
        my_capabilities_are: List[Dict[str, str]],
        nlp_model: BaseModel,
        tools=None,
    ):
        super().__init__(my_name_is, iam, my_capabilities_are, nlp_model, tools=tools)
        self.logger = logging.getLogger(self.my_name_is)
    
    async def solve_task(self, task: Task, **kwargs: Any) -> Dict:
        """
        R√©sout la t√¢che en la d√©coupant en sous-t√¢ches et renvoie directement les sous-t√¢ches.
        """
        # Construire le prompt pour d√©composer la t√¢che
        prompt = self.build_task_decomposition_prompt(task)
        
        # Appeler le mod√®le NLP pour obtenir la d√©composition
        response = await self.nlp_model.chat_completion(
            [Message(role="system", content=prompt), Message(role="user", content=task.request)],
            tools=[],
            response_format={"type": "json_object"}
        )

        # Analyser les sous-t√¢ches obtenues
        subtasks = json.loads(getattr(response.message, "content", "{}"))
        
        return subtasks


    def build_task_decomposition_prompt(self, task: Task) -> str:
        """
        Construit le prompt pour analyser une requ√™te juridique et la d√©composer en sous-probl√®mes distincts.
        
        :param task: La t√¢che que nous devons d√©couper en sous-t√¢ches.
        :return: Un prompt structur√© pour une IA d'analyse juridique.
        """
        # Filtrer les messages utiles dans l'historique de la conversation
        conversation_history = [
            {"role": step.role, "content": step.content}
            for step in task.steps
            if step.step_type in ("user_message", "assistant_message")  # Filtrage des messages pertinents
        ]

        # üîπ Conversion en JSON format√©
        formatted_history = json.dumps(conversation_history, ensure_ascii=False, indent=2)

        # üîπ Construction du prompt avec √©chappement des accolades
        prompt_template = f"""
    # Contexte
    Tu es un assistant juridique expert dans la planification et la structuration des dossiers juridiques.  
    Ton r√¥le est d‚Äôanalyser la demande utilisateur qui est une **requ√™te juridique** et son contexte afin de la d√©composer en **sous-probl√®mes juridiques distincts**.

    # Historique de la conversation
    Voici les √©changes r√©cents entre l‚Äôutilisateur et l‚Äôassistant juridique :  
    {formatted_history}

    # Objectif
    D√©compose la demande utilisateur en plusieurs sous-t√¢ches juridiques et attribue chaque sous-t√¢che √† un **expert juridique sp√©cialis√©**.  
    Si certaines parties n√©cessitent une expertise particuli√®re, pr√©cise **le domaine du droit applicable**.
    Tu ajoutes toujours une sous-tache qui mentionnent les recherches d'articles de droits ou de jurisprudences √† effectuer**.

    # Format attendu (JSON)
    Retourne une r√©ponse sous le format suivant :
    {{
    "main_legal_issue": "R√©sum√© global de la probl√©matique juridique",
    "subtasks": [
        {{
        "id": 1,
        "description": "Premi√®re sous-t√¢che juridique √† r√©soudre",
        "required_expert": "Nom du sp√©cialiste en droit concern√©",
        "applicable_law": "Code ou loi applicable"
        }},
        {{
        "id": 2,
        "description": "Deuxi√®me sous-t√¢che juridique",
        "required_expert": "Nom du sp√©cialiste en droit concern√©",
        "applicable_law": "Code ou loi applicable"
        }}
    ]
    }}
        """
        return prompt_template.strip()
