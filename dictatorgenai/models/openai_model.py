import asyncio
from openai import AsyncOpenAI
from .base_model import Message, BaseModel
from typing import Any, AsyncGenerator, Generator, List, Dict


class OpenaiModel(BaseModel):
    def __init__(self, api_key: str):
        self.client = AsyncOpenAI(api_key=api_key)

    async def chat_completion(self, messages: List[Message], tools: List[Dict] = None, **kwargs: Any) -> Dict:
        """
        Handles a chat completion with optional tool support.

        Args:
            messages (List[Message]): User and system messages.
            tools (List[Dict], optional): List of tools defined with JSON schemas.
            **kwargs (Any): Additional completion arguments.

        Returns:
            Dict: The full response generated by the model, including tool_calls if present.
        """
        completion_args = {
            "model": "gpt-4o",
            "messages": messages,
        }
        if tools:
            completion_args["tools"] = tools  # Add tools if provided
        if "response_format" in kwargs:
            completion_args["response_format"] = kwargs.pop("response_format")

        # Call the OpenAI API
        completion = await self.client.chat.completions.create(**completion_args)

        # Extract the response
        return completion.choices[0]

    async def stream_chat_completion(
        self, messages: List[Message], tools: List[Dict] = None, **kwargs: Any
    ) -> AsyncGenerator[str, None]:
        """
        Handles a streaming chat completion with optional tool support.

        Args:
            messages (List[Message]): User and system messages.
            tools (List[Dict], optional): List of tools defined with JSON schemas.
            **kwargs (Any): Additional completion arguments.

        Yields:
            str: A fragment of the response generated by the model.
        """
        completion_args = {"model": "gpt-4o", "messages": messages, "stream": True}
        if tools:
            completion_args["tools"] = tools  # Add tools if provided

        # Use async for to handle the stream asynchronously
        async for chunk in await self.client.chat.completions.create(**completion_args):
            # Check if the chunk contains content delta
            if chunk.choices and chunk.choices[0].delta.content is not None:
                yield chunk.choices[0].delta.content

    def _stream_chat_completion(
        self, messages: List[Message], tools: List[Dict] = None
    ) -> Generator[str, None, None]:
        """
        Handles a synchronous streaming chat completion with optional tool support.

        Args:
            messages (List[Message]): User and system messages.
            tools (List[Dict], optional): List of tools defined with JSON schemas.

        Yields:
            str: A fragment of the response generated by the model.
        """
        completion_args = {"model": "gpt-4o", "messages": messages, "stream": True}
        if tools:
            completion_args["tools"] = tools  # Add tools if provided

        stream = self.client.chat.completions.create(**completion_args)
        for chunk in stream:
            if chunk.choices and chunk.choices[0].delta.content is not None:
                yield chunk.choices[0].delta.content
